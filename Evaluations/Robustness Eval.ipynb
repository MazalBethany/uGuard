{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ec76b651",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "from multiprocessing import Pool\n",
    "\n",
    "\n",
    "import torchvision\n",
    "from torchvision import models as tvmodels\n",
    "from torchsummary import summary\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "\n",
    "import torchvision.models as torchvisionmodels\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import argparse\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "import itertools\n",
    "import more_itertools\n",
    "\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from captum.attr import LayerGradCam\n",
    "from captum.attr import visualization\n",
    "from PIL import Image\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "from dask_image.imread import imread\n",
    "from dask_image import ndfilters, ndmorph, ndmeasure\n",
    "import matplotlib.pyplot as plt\n",
    "from dask_image import ndmeasure\n",
    "\n",
    "from operator import itemgetter\n",
    "\n",
    "\n",
    "from torchattacks import *\n",
    "import torchattacks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9655e1b6",
   "metadata": {},
   "source": [
    "We will test on two attacks that were trained on\n",
    "1. BIM\n",
    "2. Square\n",
    "\n",
    "\n",
    "We will test on two attacks that were not trained on\n",
    "1. Autoattack / DeepFool\n",
    "2. CW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2781804",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Autoattack robustness testing\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f78342",
   "metadata": {},
   "source": [
    "## NSFW Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21ee03c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/ocistudent/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model import resnet\n",
    "#from model.utils import load_filtered_state_dict, SaveBestModel, AverageMeter, accuracy\n",
    "# model = resnet.ResNet(torchvision.models.resnet.Bottleneck, [3, 4, 6, 3], 2)\n",
    "# checkpoint = torch.load('nsfw-pytorch/models/nsfw/epoch_2.pkl')\n",
    "# load_filtered_state_dict(model, checkpoint, ignore_layer=[], reverse=True)\n",
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', pretrained=True)\n",
    "model.fc = nn.Linear(2048, 2, bias=True)\n",
    "checkpoint = torch.load('non_robust_models/ResNet_NSFW.pth')\n",
    "#model.load_state_dict(checkpoint, strict=False)\n",
    "transform_test = transforms.Compose([transforms.Resize(320),\n",
    "        transforms.RandomCrop(299), transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "good_img_transform = transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "classes = {0:'neutral', 1:'porn'}\n",
    "\n",
    "test_path = 'nsfw/test'\n",
    "testset = torchvision.datasets.ImageFolder(test_path, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=1, shuffle=True, num_workers=2)\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "model.load_state_dict(checkpoint, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3c9a8337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9801324503311258\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total_samples = 0\n",
    "\n",
    "for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "    inputs, targets = inputs, targets\n",
    "    #print(type(targets.item()))\n",
    "    #adv_image = atk(inputs, targets)\n",
    "    \n",
    "    label = targets.item()\n",
    "        \n",
    "    outputs = model(inputs.to(device))\n",
    "    _, predicted = outputs.max(1)\n",
    "    pred_val = predicted.item()\n",
    "    \n",
    "    total_samples += 1\n",
    "    if pred_val == label:\n",
    "        correct += 1\n",
    "    \n",
    "    if total_samples > 150:\n",
    "        break\n",
    "    \n",
    "print(correct / total_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "783a6ba9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f5bc805fab0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "#labels = 2\n",
    "\n",
    "#atk = torchattacks.PGD(model, eps=16/255, alpha=2/255, steps=100, random_start=True)\n",
    "#atk = torchattacks.Square(model, eps=16/255, n_queries=500, n_restarts=1, loss='ce')\n",
    "#atk = torchattacks.Square(model, norm='Linf', n_queries=500, n_restarts=1, eps=16/255, p_init=.8, seed=0, verbose=False, loss='margin', resc_schedule=True)\n",
    "#atk = torchattacks.AutoAttack(model, eps=8/255, n_classes=2, version='standard')\n",
    "#atk = torchattacks.PGD(model, eps=8/255, alpha=2/255, steps=100, random_start=True)\n",
    "#atk = torchattacks.CW(model, c=1e-4, kappa=0, steps=100, lr=0.01)\n",
    "#atk = torchattacks.DeepFool(model, steps=50, overshoot=0.02)\n",
    "#atk = torchattacks.BIM(model, eps=4/255, alpha=1/255, steps=10)\n",
    "#adv_image = atk(images, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae9b29c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25165562913907286\n"
     ]
    }
   ],
   "source": [
    "# Attack 1\n",
    "atk = torchattacks.BIM(model, eps=4/255, alpha=1/255, steps=10)\n",
    "correct = 0\n",
    "total_samples = 0\n",
    "\n",
    "for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "    inputs, targets = inputs, targets\n",
    "    #print(type(targets.item()))\n",
    "    adv_image = atk(inputs, targets)\n",
    "    \n",
    "    label = targets.item()\n",
    "        \n",
    "    outputs = model(adv_image.to(device))\n",
    "    _, predicted = outputs.max(1)\n",
    "    pred_val = predicted.item()\n",
    "    \n",
    "    total_samples += 1\n",
    "    if pred_val == label:\n",
    "        correct += 1\n",
    "    \n",
    "    if total_samples > 150:\n",
    "        break\n",
    "    \n",
    "print(correct / total_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57f881c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6357615894039735\n"
     ]
    }
   ],
   "source": [
    "# Attack 2\n",
    "atk = torchattacks.Square(model, norm='Linf', n_queries=500, n_restarts=1, eps=16/255, p_init=.8, seed=0, verbose=False, loss='margin', resc_schedule=True)\n",
    "correct = 0\n",
    "total_samples = 0\n",
    "\n",
    "for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "    inputs, targets = inputs, targets\n",
    "    #print(type(targets.item()))\n",
    "    adv_image = atk(inputs, targets)\n",
    "    \n",
    "    label = targets.item()\n",
    "        \n",
    "    outputs = model(adv_image.to(device))\n",
    "    _, predicted = outputs.max(1)\n",
    "    pred_val = predicted.item()\n",
    "    \n",
    "    total_samples += 1\n",
    "    if pred_val == label:\n",
    "        correct += 1\n",
    "    \n",
    "    if total_samples > 150:\n",
    "        break\n",
    "    \n",
    "print(correct / total_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59a5a628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12582781456953643\n"
     ]
    }
   ],
   "source": [
    "# Attack 3\n",
    "atk = torchattacks.DeepFool(model, steps=50, overshoot=0.02)\n",
    "correct = 0\n",
    "total_samples = 0\n",
    "\n",
    "for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "    inputs, targets = inputs, targets\n",
    "    #print(type(targets.item()))\n",
    "    adv_image = atk(inputs, targets)\n",
    "    \n",
    "    label = targets.item()\n",
    "        \n",
    "    outputs = model(adv_image.to(device))\n",
    "    _, predicted = outputs.max(1)\n",
    "    pred_val = predicted.item()\n",
    "    \n",
    "    total_samples += 1\n",
    "    if pred_val == label:\n",
    "        correct += 1\n",
    "    \n",
    "    if total_samples > 150:\n",
    "        break\n",
    "    \n",
    "print(correct / total_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0439ddb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6490066225165563\n"
     ]
    }
   ],
   "source": [
    "# Attack 4\n",
    "atk = torchattacks.CW(model, c=1e-4, kappa=0, steps=100, lr=0.01)\n",
    "correct = 0\n",
    "total_samples = 0\n",
    "\n",
    "for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "    inputs, targets = inputs, targets\n",
    "    #print(type(targets.item()))\n",
    "    adv_image = atk(inputs, targets)\n",
    "    \n",
    "    label = targets.item()\n",
    "        \n",
    "    outputs = model(adv_image.to(device))\n",
    "    _, predicted = outputs.max(1)\n",
    "    pred_val = predicted.item()\n",
    "    \n",
    "    total_samples += 1\n",
    "    if pred_val == label:\n",
    "        correct += 1\n",
    "    \n",
    "    if total_samples > 150:\n",
    "        break\n",
    "    \n",
    "print(correct / total_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e1e069f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# Attack 5\n",
    "atk = torchattacks.AutoAttack(model, eps=8/255, n_classes=2, version='standard')\n",
    "correct = 0\n",
    "total_samples = 0\n",
    "\n",
    "for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "\n",
    "    inputs, targets = inputs, targets\n",
    "    #print(inputs.size())\n",
    "    try:\n",
    "        adv_image = atk(inputs, targets)\n",
    "    \n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    label = targets.item()\n",
    "        \n",
    "    outputs = model(adv_image.to(device))\n",
    "    _, predicted = outputs.max(1)\n",
    "    pred_val = predicted.item()\n",
    "    \n",
    "    total_samples += 1\n",
    "    if pred_val == label:\n",
    "        correct += 1\n",
    "    \n",
    "    if total_samples > 150:\n",
    "        break\n",
    "    \n",
    "print(correct / total_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a216148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5960264900662252\n"
     ]
    }
   ],
   "source": [
    "# Combination\n",
    "\n",
    "correct = 0\n",
    "total_samples = 0\n",
    "\n",
    "for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "    inputs, targets = inputs, targets\n",
    "    #print(type(targets.item()))\n",
    "    atk1 = torchattacks.BIM(model, eps=4/255, alpha=1/255, steps=10)\n",
    "    adv_image1 = atk1(inputs, targets)\n",
    "    atk2 = torchattacks.DeepFool(model, steps=50, overshoot=0.02)\n",
    "    adv_image2 = atk2(inputs, targets)\n",
    "    \n",
    "    add = adv_image1.add(adv_image2)\n",
    "    \n",
    "    adv_image = torch.div(add, 2)\n",
    "    \n",
    "    label = targets.item()\n",
    "        \n",
    "    outputs = model(adv_image.to(device))\n",
    "    _, predicted = outputs.max(1)\n",
    "    pred_val = predicted.item()\n",
    "    \n",
    "    total_samples += 1\n",
    "    if pred_val == label:\n",
    "        correct += 1\n",
    "    \n",
    "    if total_samples > 150:\n",
    "        break\n",
    "    \n",
    "print(correct / total_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e45bb4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "af062c5d",
   "metadata": {},
   "source": [
    "## CB Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee9e9d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/ocistudent/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model import resnet\n",
    "#from model.utils import load_filtered_state_dict, SaveBestModel, AverageMeter, accuracy\n",
    "# model = resnet.ResNet(torchvision.models.resnet.Bottleneck, [3, 4, 6, 3], 2)\n",
    "# checkpoint = torch.load('nsfw-pytorch/models/nsfw/epoch_2.pkl')\n",
    "# load_filtered_state_dict(model, checkpoint, ignore_layer=[], reverse=True)\n",
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', pretrained=True)\n",
    "model.fc = nn.Linear(2048, 2, bias=True)\n",
    "checkpoint = torch.load('non_robust_models/ResNet_CB.pth')\n",
    "#model.load_state_dict(checkpoint, strict=False)\n",
    "transform_test = transforms.Compose([transforms.Resize(320),\n",
    "        transforms.RandomCrop(299), transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "good_img_transform = transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "classes = {0:'cyberbullying', 1:'non_cyberbullying'}\n",
    "\n",
    "test_path = 'cyberbullying/test'\n",
    "testset = torchvision.datasets.ImageFolder(test_path, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=1, shuffle=True, num_workers=2)\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "model.load_state_dict(checkpoint, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "bce89fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9139072847682119\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total_samples = 0\n",
    "\n",
    "for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "    inputs, targets = inputs, targets\n",
    "    #print(type(targets.item()))\n",
    "    #adv_image = atk(inputs, targets)\n",
    "    \n",
    "    label = targets.item()\n",
    "        \n",
    "    outputs = model(inputs.to(device))\n",
    "    _, predicted = outputs.max(1)\n",
    "    pred_val = predicted.item()\n",
    "    \n",
    "    total_samples += 1\n",
    "    if pred_val == label:\n",
    "        correct += 1\n",
    "    \n",
    "    if total_samples > 150:\n",
    "        break\n",
    "    \n",
    "print(correct / total_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0ab35c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f5bc805fab0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "#labels = 2\n",
    "\n",
    "#atk = torchattacks.PGD(model, eps=16/255, alpha=2/255, steps=100, random_start=True)\n",
    "#atk = torchattacks.Square(model, eps=16/255, n_queries=500, n_restarts=1, loss='ce')\n",
    "#atk = torchattacks.AutoAttack(model, eps=8/255, n_classes=2, version='standard')\n",
    "#atk = torchattacks.PGD(model, eps=8/255, alpha=2/255, steps=100, random_start=True)\n",
    "#adv_image = atk(images, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6efbe2a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48344370860927155\n"
     ]
    }
   ],
   "source": [
    "# Attack 1\n",
    "atk = torchattacks.BIM(model, eps=4/255, alpha=1/255, steps=10)\n",
    "correct = 0\n",
    "total_samples = 0\n",
    "\n",
    "for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "    inputs, targets = inputs, targets\n",
    "    #print(type(targets.item()))\n",
    "    adv_image = atk(inputs, targets)\n",
    "    \n",
    "    label = targets.item()\n",
    "        \n",
    "    outputs = model(adv_image.to(device))\n",
    "    _, predicted = outputs.max(1)\n",
    "    pred_val = predicted.item()\n",
    "    \n",
    "    total_samples += 1\n",
    "    if pred_val == label:\n",
    "        correct += 1\n",
    "    \n",
    "    if total_samples > 150:\n",
    "        break\n",
    "    \n",
    "print(correct / total_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fde144f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ocistudent/ocistudent/envs/SMU/lib/python3.7/site-packages/PIL/Image.py:993: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5695364238410596\n"
     ]
    }
   ],
   "source": [
    "# Attack 2\n",
    "atk = torchattacks.Square(model, norm='Linf', n_queries=500, n_restarts=1, eps=16/255, p_init=.8, seed=0, verbose=False, loss='margin', resc_schedule=True)\n",
    "correct = 0\n",
    "total_samples = 0\n",
    "\n",
    "for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "    inputs, targets = inputs, targets\n",
    "    #print(type(targets.item()))\n",
    "    adv_image = atk(inputs, targets)\n",
    "    \n",
    "    label = targets.item()\n",
    "        \n",
    "    outputs = model(adv_image.to(device))\n",
    "    _, predicted = outputs.max(1)\n",
    "    pred_val = predicted.item()\n",
    "    \n",
    "    total_samples += 1\n",
    "    if pred_val == label:\n",
    "        correct += 1\n",
    "    \n",
    "    if total_samples > 150:\n",
    "        break\n",
    "    \n",
    "print(correct / total_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12c296b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.052980132450331126\n"
     ]
    }
   ],
   "source": [
    "# Attack 3\n",
    "atk = torchattacks.DeepFool(model, steps=50, overshoot=0.02)\n",
    "correct = 0\n",
    "total_samples = 0\n",
    "\n",
    "for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "    inputs, targets = inputs, targets\n",
    "    #print(type(targets.item()))\n",
    "    adv_image = atk(inputs, targets)\n",
    "    \n",
    "    label = targets.item()\n",
    "        \n",
    "    outputs = model(adv_image.to(device))\n",
    "    _, predicted = outputs.max(1)\n",
    "    pred_val = predicted.item()\n",
    "    \n",
    "    total_samples += 1\n",
    "    if pred_val == label:\n",
    "        correct += 1\n",
    "    \n",
    "    if total_samples > 150:\n",
    "        break\n",
    "    \n",
    "print(correct / total_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94f71c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7682119205298014\n"
     ]
    }
   ],
   "source": [
    "# Attack 4\n",
    "atk = torchattacks.CW(model, c=1e-4, kappa=0, steps=100, lr=0.01)\n",
    "correct = 0\n",
    "total_samples = 0\n",
    "\n",
    "for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "    inputs, targets = inputs, targets\n",
    "    #print(type(targets.item()))\n",
    "    adv_image = atk(inputs, targets)\n",
    "    \n",
    "    label = targets.item()\n",
    "        \n",
    "    outputs = model(adv_image.to(device))\n",
    "    _, predicted = outputs.max(1)\n",
    "    pred_val = predicted.item()\n",
    "    \n",
    "    total_samples += 1\n",
    "    if pred_val == label:\n",
    "        correct += 1\n",
    "    \n",
    "    if total_samples > 150:\n",
    "        break\n",
    "    \n",
    "print(correct / total_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45f512fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.019867549668874173\n"
     ]
    }
   ],
   "source": [
    "# Attack 5\n",
    "atk = torchattacks.AutoAttack(model, eps=8/255, n_classes=2, version='standard')\n",
    "correct = 0\n",
    "total_samples = 0\n",
    "\n",
    "for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "\n",
    "    inputs, targets = inputs, targets\n",
    "    #print(inputs.size())\n",
    "    try:\n",
    "        adv_image = atk(inputs, targets)\n",
    "    \n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    label = targets.item()\n",
    "        \n",
    "    outputs = model(adv_image.to(device))\n",
    "    _, predicted = outputs.max(1)\n",
    "    pred_val = predicted.item()\n",
    "    \n",
    "    total_samples += 1\n",
    "    if pred_val == label:\n",
    "        correct += 1\n",
    "    \n",
    "    if total_samples > 150:\n",
    "        break\n",
    "    \n",
    "print(correct / total_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "592b964f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.695364238410596\n"
     ]
    }
   ],
   "source": [
    "# Combination\n",
    "\n",
    "correct = 0\n",
    "total_samples = 0\n",
    "\n",
    "for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "    inputs, targets = inputs, targets\n",
    "    #print(type(targets.item()))\n",
    "    atk1 = torchattacks.BIM(model, eps=4/255, alpha=1/255, steps=10)\n",
    "    adv_image1 = atk1(inputs, targets)\n",
    "    atk2 = torchattacks.DeepFool(model, steps=50, overshoot=0.02)\n",
    "    adv_image2 = atk2(inputs, targets)\n",
    "    \n",
    "    add = adv_image1.add(adv_image2)\n",
    "    \n",
    "    adv_image = torch.div(add, 2)\n",
    "    \n",
    "    label = targets.item()\n",
    "        \n",
    "    outputs = model(adv_image.to(device))\n",
    "    _, predicted = outputs.max(1)\n",
    "    pred_val = predicted.item()\n",
    "    \n",
    "    total_samples += 1\n",
    "    if pred_val == label:\n",
    "        correct += 1\n",
    "    \n",
    "    if total_samples > 150:\n",
    "        break\n",
    "    \n",
    "print(correct / total_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfbfd2f",
   "metadata": {},
   "source": [
    "## Self-Harm Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b46caae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/ocistudent/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model import resnet\n",
    "#from model.utils import load_filtered_state_dict, SaveBestModel, AverageMeter, accuracy\n",
    "# model = resnet.ResNet(torchvision.models.resnet.Bottleneck, [3, 4, 6, 3], 2)\n",
    "# checkpoint = torch.load('nsfw-pytorch/models/nsfw/epoch_2.pkl')\n",
    "# load_filtered_state_dict(model, checkpoint, ignore_layer=[], reverse=True)\n",
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', pretrained=True)\n",
    "model.fc = nn.Linear(2048, 2, bias=True)\n",
    "checkpoint = torch.load('non_robust_models/ResNet_SH.pth')\n",
    "#model.load_state_dict(checkpoint, strict=False)\n",
    "transform_test = transforms.Compose([transforms.Resize(320),\n",
    "        transforms.RandomCrop(299), transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "good_img_transform = transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "classes = {0:'neutral', 1:'porn'}\n",
    "\n",
    "test_path = 'self_harm/test'\n",
    "testset = torchvision.datasets.ImageFolder(test_path, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=1, shuffle=True, num_workers=2)\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "model.load_state_dict(checkpoint, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7e41ccff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9470198675496688\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total_samples = 0\n",
    "\n",
    "for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "    inputs, targets = inputs, targets\n",
    "    #print(type(targets.item()))\n",
    "    #adv_image = atk(inputs, targets)\n",
    "    \n",
    "    label = targets.item()\n",
    "        \n",
    "    outputs = model(inputs.to(device))\n",
    "    _, predicted = outputs.max(1)\n",
    "    pred_val = predicted.item()\n",
    "    \n",
    "    total_samples += 1\n",
    "    if pred_val == label:\n",
    "        correct += 1\n",
    "    \n",
    "    if total_samples > 150:\n",
    "        break\n",
    "    \n",
    "print(correct / total_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "48b0b8d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f5bc805fab0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "#labels = 2\n",
    "\n",
    "#atk = torchattacks.PGD(model, eps=16/255, alpha=2/255, steps=100, random_start=True)\n",
    "#atk = torchattacks.Square(model, eps=16/255, n_queries=500, n_restarts=1, loss='ce')\n",
    "#atk = torchattacks.AutoAttack(model, eps=8/255, n_classes=2, version='standard')\n",
    "#atk = torchattacks.PGD(model, eps=8/255, alpha=2/255, steps=100, random_start=True)\n",
    "#adv_image = atk(images, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f698540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5231788079470199\n"
     ]
    }
   ],
   "source": [
    "# Attack 1\n",
    "atk = torchattacks.BIM(model, eps=4/255, alpha=1/255, steps=10)\n",
    "correct = 0\n",
    "total_samples = 0\n",
    "\n",
    "for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "    inputs, targets = inputs, targets\n",
    "    #print(type(targets.item()))\n",
    "    adv_image = atk(inputs, targets)\n",
    "    \n",
    "    label = targets.item()\n",
    "        \n",
    "    outputs = model(adv_image.to(device))\n",
    "    _, predicted = outputs.max(1)\n",
    "    pred_val = predicted.item()\n",
    "    \n",
    "    total_samples += 1\n",
    "    if pred_val == label:\n",
    "        correct += 1\n",
    "    \n",
    "    if total_samples > 150:\n",
    "        break\n",
    "    \n",
    "print(correct / total_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b8985b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5364238410596026\n"
     ]
    }
   ],
   "source": [
    "# Attack 2\n",
    "atk = torchattacks.Square(model, norm='Linf', n_queries=500, n_restarts=1, eps=16/255, p_init=.8, seed=0, verbose=False, loss='margin', resc_schedule=True)\n",
    "correct = 0\n",
    "total_samples = 0\n",
    "\n",
    "for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "    inputs, targets = inputs, targets\n",
    "    #print(type(targets.item()))\n",
    "    adv_image = atk(inputs, targets)\n",
    "    \n",
    "    label = targets.item()\n",
    "        \n",
    "    outputs = model(adv_image.to(device))\n",
    "    _, predicted = outputs.max(1)\n",
    "    pred_val = predicted.item()\n",
    "    \n",
    "    total_samples += 1\n",
    "    if pred_val == label:\n",
    "        correct += 1\n",
    "    \n",
    "    if total_samples > 150:\n",
    "        break\n",
    "    \n",
    "print(correct / total_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b40114dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.052980132450331126\n"
     ]
    }
   ],
   "source": [
    "# Attack 3\n",
    "atk = torchattacks.DeepFool(model, steps=50, overshoot=0.02)\n",
    "correct = 0\n",
    "total_samples = 0\n",
    "\n",
    "for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "    inputs, targets = inputs, targets\n",
    "    #print(type(targets.item()))\n",
    "    adv_image = atk(inputs, targets)\n",
    "    \n",
    "    label = targets.item()\n",
    "        \n",
    "    outputs = model(adv_image.to(device))\n",
    "    _, predicted = outputs.max(1)\n",
    "    pred_val = predicted.item()\n",
    "    \n",
    "    total_samples += 1\n",
    "    if pred_val == label:\n",
    "        correct += 1\n",
    "    \n",
    "    if total_samples > 150:\n",
    "        break\n",
    "    \n",
    "print(correct / total_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c67b0f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5231788079470199\n"
     ]
    }
   ],
   "source": [
    "# Attack 4\n",
    "atk = torchattacks.CW(model, c=1e-4, kappa=0, steps=100, lr=0.01)\n",
    "correct = 0\n",
    "total_samples = 0\n",
    "\n",
    "for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "    inputs, targets = inputs, targets\n",
    "    #print(type(targets.item()))\n",
    "    adv_image = atk(inputs, targets)\n",
    "    \n",
    "    label = targets.item()\n",
    "        \n",
    "    outputs = model(adv_image.to(device))\n",
    "    _, predicted = outputs.max(1)\n",
    "    pred_val = predicted.item()\n",
    "    \n",
    "    total_samples += 1\n",
    "    if pred_val == label:\n",
    "        correct += 1\n",
    "    \n",
    "    if total_samples > 150:\n",
    "        break\n",
    "    \n",
    "print(correct / total_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a169fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.052980132450331126\n"
     ]
    }
   ],
   "source": [
    "# Attack 5\n",
    "atk = torchattacks.AutoAttack(model, eps=8/255, n_classes=2, version='standard')\n",
    "correct = 0\n",
    "total_samples = 0\n",
    "\n",
    "for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "\n",
    "    inputs, targets = inputs, targets\n",
    "    #print(inputs.size())\n",
    "    try:\n",
    "        adv_image = atk(inputs, targets)\n",
    "    \n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    label = targets.item()\n",
    "        \n",
    "    outputs = model(adv_image.to(device))\n",
    "    _, predicted = outputs.max(1)\n",
    "    pred_val = predicted.item()\n",
    "    \n",
    "    total_samples += 1\n",
    "    if pred_val == label:\n",
    "        correct += 1\n",
    "    \n",
    "    if total_samples > 150:\n",
    "        break\n",
    "    \n",
    "print(correct / total_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ccd1fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7218543046357616\n"
     ]
    }
   ],
   "source": [
    "# Combination\n",
    "\n",
    "correct = 0\n",
    "total_samples = 0\n",
    "\n",
    "for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "    inputs, targets = inputs, targets\n",
    "    #print(type(targets.item()))\n",
    "    atk1 = torchattacks.BIM(model, eps=4/255, alpha=1/255, steps=10)\n",
    "    adv_image1 = atk1(inputs, targets)\n",
    "    atk2 = torchattacks.DeepFool(model, steps=50, overshoot=0.02)\n",
    "    adv_image2 = atk2(inputs, targets)\n",
    "    \n",
    "    add = adv_image1.add(adv_image2)\n",
    "    \n",
    "    adv_image = torch.div(add, 2)\n",
    "    \n",
    "    label = targets.item()\n",
    "        \n",
    "    outputs = model(adv_image.to(device))\n",
    "    _, predicted = outputs.max(1)\n",
    "    pred_val = predicted.item()\n",
    "    \n",
    "    total_samples += 1\n",
    "    if pred_val == label:\n",
    "        correct += 1\n",
    "    \n",
    "    if total_samples > 150:\n",
    "        break\n",
    "    \n",
    "print(correct / total_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c18f6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "05f72f8a",
   "metadata": {},
   "source": [
    "## Create Datasets of Attacked Images to be used in Purification Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace42e40",
   "metadata": {},
   "source": [
    "### NSFW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9988e771",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/ocistudent/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model import resnet\n",
    "#from model.utils import load_filtered_state_dict, SaveBestModel, AverageMeter, accuracy\n",
    "# model = resnet.ResNet(torchvision.models.resnet.Bottleneck, [3, 4, 6, 3], 2)\n",
    "# checkpoint = torch.load('nsfw-pytorch/models/nsfw/epoch_2.pkl')\n",
    "# load_filtered_state_dict(model, checkpoint, ignore_layer=[], reverse=True)\n",
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', pretrained=True)\n",
    "model.fc = nn.Linear(2048, 2, bias=True)\n",
    "checkpoint = torch.load('non_robust_models/ResNet_NSFW.pth')\n",
    "#model.load_state_dict(checkpoint, strict=False)\n",
    "transform_test = transforms.Compose([transforms.Resize(320),\n",
    "        transforms.RandomCrop(299), transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "good_img_transform = transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "classes = {0:'neutral', 1:'porn'}\n",
    "\n",
    "test_path = 'nsfw/test'\n",
    "testset = torchvision.datasets.ImageFolder(test_path, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=1, shuffle=True, num_workers=2)\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "model.load_state_dict(checkpoint, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "06cbf90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "\n",
    "for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "        inputs, targets = inputs, targets\n",
    "        #print(type(targets.item()))\n",
    "        if targets.item() == 0:\n",
    "            torchvision.utils.save_image(inputs, 'non_robust_models/NSFW_attacked/clean'+'/neutral/'+str(i)+'.png')\n",
    "            \n",
    "        if targets.item() == 1:\n",
    "            torchvision.utils.save_image(inputs, 'non_robust_models/NSFW_attacked/clean'+'/porn/'+str(i)+'.png')\n",
    "         \n",
    "        i += 1\n",
    "        if i > 400:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cf129e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "\n",
    "\n",
    "for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "        inputs, targets = inputs, targets\n",
    "        #print(type(targets.item()))\n",
    "        adv_image = atk(inputs, targets)\n",
    "        if targets.item() == 0:\n",
    "            #BIM\n",
    "            atk = torchattacks.BIM(model, eps=4/255, alpha=1/255, steps=10)\n",
    "            adv_image = atk(inputs, targets)\n",
    "            torchvision.utils.save_image(adv_image, 'non_robust_models/NSFW_attacked/BIM'+'/neutral/'+str(i)+'.png')\n",
    "            #Square\n",
    "            atk = torchattacks.Square(model, norm='Linf', n_queries=500, n_restarts=1, eps=16/255, p_init=.8, seed=0, verbose=False, loss='margin', resc_schedule=True)\n",
    "            adv_image = atk(inputs, targets)\n",
    "            torchvision.utils.save_image(adv_image, 'non_robust_models/NSFW_attacked/Square'+'/neutral/'+str(i)+'.png')\n",
    "            #DF\n",
    "            atk = torchattacks.DeepFool(model, steps=50, overshoot=0.02)\n",
    "            adv_image = atk(inputs, targets)\n",
    "            torchvision.utils.save_image(adv_image, 'non_robust_models/NSFW_attacked/DF'+'/neutral/'+str(i)+'.png')\n",
    "            #CW\n",
    "            atk = torchattacks.CW(model, c=1e-4, kappa=0, steps=100, lr=0.01)\n",
    "            adv_image = atk(inputs, targets)\n",
    "            torchvision.utils.save_image(adv_image, 'non_robust_models/NSFW_attacked/CW'+'/neutral/'+str(i)+'.png')\n",
    "            \n",
    "        if targets.item() == 1:\n",
    "            #BIM\n",
    "            atk = torchattacks.BIM(model, eps=4/255, alpha=1/255, steps=10)\n",
    "            adv_image = atk(inputs, targets)\n",
    "            torchvision.utils.save_image(adv_image, 'non_robust_models/NSFW_attacked/BIM'+'/porn/'+str(i)+'.png')\n",
    "            #Square\n",
    "            atk = torchattacks.Square(model, norm='Linf', n_queries=500, n_restarts=1, eps=16/255, p_init=.8, seed=0, verbose=False, loss='margin', resc_schedule=True)\n",
    "            adv_image = atk(inputs, targets)\n",
    "            torchvision.utils.save_image(adv_image, 'non_robust_models/NSFW_attacked/Square'+'/porn/'+str(i)+'.png')\n",
    "            #DF\n",
    "            atk = torchattacks.DeepFool(model, steps=50, overshoot=0.02)\n",
    "            adv_image = atk(inputs, targets)\n",
    "            torchvision.utils.save_image(adv_image, 'non_robust_models/NSFW_attacked/DF'+'/porn/'+str(i)+'.png')\n",
    "            #CW\n",
    "            atk = torchattacks.CW(model, c=1e-4, kappa=0, steps=100, lr=0.01)\n",
    "            adv_image = atk(inputs, targets)\n",
    "            torchvision.utils.save_image(adv_image, 'non_robust_models/NSFW_attacked/CW'+'/porn/'+str(i)+'.png')\n",
    "                    \n",
    "        i += 1\n",
    "        if i > 400:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "841cace4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Auto\n",
    "\n",
    "i = 0\n",
    "\n",
    "\n",
    "for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "        inputs, targets = inputs, targets\n",
    "        #print(type(targets.item()))\n",
    "\n",
    "        #adv_image = atk(inputs, targets)\n",
    "        if targets.item() == 0:\n",
    "            #Auto\n",
    "            atk = torchattacks.AutoAttack(model, eps=8/255, n_classes=2, version='standard')\n",
    "            try:\n",
    "                adv_image = atk(inputs, targets)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            torchvision.utils.save_image(adv_image, 'non_robust_models/NSFW_attacked/auto'+'/neutral/'+str(i)+'.png')\n",
    "\n",
    "        \n",
    "        \n",
    "        if targets.item() == 1:\n",
    "            #Auto\n",
    "            atk = torchattacks.AutoAttack(model, eps=8/255, n_classes=2, version='standard')\n",
    "            try:\n",
    "                adv_image = atk(inputs, targets)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            torchvision.utils.save_image(adv_image, 'non_robust_models/NSFW_attacked/auto'+'/porn/'+str(i)+'.png')\n",
    "\n",
    "                    \n",
    "        i += 1\n",
    "        if i > 400:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c4ffcc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combination\n",
    "\n",
    "i = 0\n",
    "\n",
    "\n",
    "for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "        inputs, targets = inputs, targets\n",
    "        #print(type(targets.item()))\n",
    "\n",
    "        #adv_image = atk(inputs, targets)\n",
    "        if targets.item() == 0:\n",
    "            atk1 = torchattacks.BIM(model, eps=4/255, alpha=1/255, steps=10)\n",
    "            adv_image1 = atk1(inputs, targets)\n",
    "            atk2 = torchattacks.DeepFool(model, steps=50, overshoot=0.02)\n",
    "            adv_image2 = atk2(inputs, targets)\n",
    "    \n",
    "            add = adv_image1.add(adv_image2)\n",
    "    \n",
    "            adv_image = torch.div(add, 2)\n",
    "            \n",
    "            torchvision.utils.save_image(adv_image, 'non_robust_models/NSFW_attacked/comb'+'/neutral/'+str(i)+'.png')\n",
    "\n",
    "        \n",
    "        \n",
    "        if targets.item() == 1:\n",
    "            atk1 = torchattacks.BIM(model, eps=4/255, alpha=1/255, steps=10)\n",
    "            adv_image1 = atk1(inputs, targets)\n",
    "            atk2 = torchattacks.DeepFool(model, steps=50, overshoot=0.02)\n",
    "            adv_image2 = atk2(inputs, targets)\n",
    "    \n",
    "            add = adv_image1.add(adv_image2)\n",
    "    \n",
    "            adv_image = torch.div(add, 2)\n",
    "            \n",
    "            torchvision.utils.save_image(adv_image, 'non_robust_models/NSFW_attacked/comb'+'/porn/'+str(i)+'.png')\n",
    "\n",
    "                    \n",
    "        i += 1\n",
    "        if i > 400:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f2d19f",
   "metadata": {},
   "source": [
    "### Cyberbullying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "565c7302",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/ocistudent/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model import resnet\n",
    "#from model.utils import load_filtered_state_dict, SaveBestModel, AverageMeter, accuracy\n",
    "# model = resnet.ResNet(torchvision.models.resnet.Bottleneck, [3, 4, 6, 3], 2)\n",
    "# checkpoint = torch.load('nsfw-pytorch/models/nsfw/epoch_2.pkl')\n",
    "# load_filtered_state_dict(model, checkpoint, ignore_layer=[], reverse=True)\n",
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', pretrained=True)\n",
    "model.fc = nn.Linear(2048, 2, bias=True)\n",
    "checkpoint = torch.load('non_robust_models/ResNet_CB.pth')\n",
    "#model.load_state_dict(checkpoint, strict=False)\n",
    "transform_test = transforms.Compose([transforms.Resize(320),\n",
    "        transforms.RandomCrop(299), transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "good_img_transform = transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "classes = {0:'cyberbullying', 1:'non_cyberbullying'}\n",
    "\n",
    "test_path = 'cyberbullying/test'\n",
    "testset = torchvision.datasets.ImageFolder(test_path, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=1, shuffle=True, num_workers=2)\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "model.load_state_dict(checkpoint, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "163b5f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "\n",
    "for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "        inputs, targets = inputs, targets\n",
    "        #print(type(targets.item()))\n",
    "        if targets.item() == 0:\n",
    "             torchvision.utils.save_image(inputs, 'non_robust_models/cyberbullying_attacked/clean'+'/cyberbullying/'+str(i)+'.png')\n",
    "            \n",
    "        if targets.item() == 1:\n",
    "             torchvision.utils.save_image(inputs, 'non_robust_models/cyberbullying_attacked/clean'+'/non_cyberbullying/'+str(i)+'.png')\n",
    "         \n",
    "        i += 1\n",
    "        if i > 400:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0e550b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ocistudent/ocistudent/envs/SMU/lib/python3.7/site-packages/PIL/Image.py:993: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "\n",
    "\n",
    "for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "        inputs, targets = inputs, targets\n",
    "        #print(type(targets.item()))\n",
    "        adv_image = atk(inputs, targets)\n",
    "        if targets.item() == 1:\n",
    "            #BIM\n",
    "            atk = torchattacks.BIM(model, eps=4/255, alpha=1/255, steps=10)\n",
    "            adv_image = atk(inputs, targets)\n",
    "            torchvision.utils.save_image(adv_image, 'non_robust_models/cyberbullying_attacked/BIM'+'/non_cyberbullying/'+str(i)+'.png')\n",
    "            #Square\n",
    "            atk = torchattacks.Square(model, norm='Linf', n_queries=500, n_restarts=1, eps=16/255, p_init=.8, seed=0, verbose=False, loss='margin', resc_schedule=True)\n",
    "            adv_image = atk(inputs, targets)\n",
    "            torchvision.utils.save_image(adv_image, 'non_robust_models/cyberbullying_attacked/Square'+'/non_cyberbullying/'+str(i)+'.png')\n",
    "            #DF\n",
    "            atk = torchattacks.DeepFool(model, steps=50, overshoot=0.02)\n",
    "            adv_image = atk(inputs, targets)\n",
    "            torchvision.utils.save_image(adv_image, 'non_robust_models/cyberbullying_attacked/DF'+'/non_cyberbullying/'+str(i)+'.png')\n",
    "            #CW\n",
    "            atk = torchattacks.CW(model, c=1e-4, kappa=0, steps=100, lr=0.01)\n",
    "            adv_image = atk(inputs, targets)\n",
    "            torchvision.utils.save_image(adv_image, 'non_robust_models/cyberbullying_attacked/CW'+'/non_cyberbullying/'+str(i)+'.png')\n",
    "            \n",
    "        if targets.item() == 0:\n",
    "            #BIM\n",
    "            atk = torchattacks.BIM(model, eps=4/255, alpha=1/255, steps=10)\n",
    "            adv_image = atk(inputs, targets)\n",
    "            torchvision.utils.save_image(adv_image, 'non_robust_models/cyberbullying_attacked/BIM'+'/cyberbullying/'+str(i)+'.png')\n",
    "            #Square\n",
    "            atk = torchattacks.Square(model, norm='Linf', n_queries=500, n_restarts=1, eps=16/255, p_init=.8, seed=0, verbose=False, loss='margin', resc_schedule=True)\n",
    "            adv_image = atk(inputs, targets)\n",
    "            torchvision.utils.save_image(adv_image, 'non_robust_models/cyberbullying_attacked/Square'+'/cyberbullying/'+str(i)+'.png')\n",
    "            #DF\n",
    "            atk = torchattacks.DeepFool(model, steps=50, overshoot=0.02)\n",
    "            adv_image = atk(inputs, targets)\n",
    "            torchvision.utils.save_image(adv_image, 'non_robust_models/cyberbullying_attacked/DF'+'/cyberbullying/'+str(i)+'.png')\n",
    "            #CW\n",
    "            atk = torchattacks.CW(model, c=1e-4, kappa=0, steps=100, lr=0.01)\n",
    "            adv_image = atk(inputs, targets)\n",
    "            torchvision.utils.save_image(adv_image, 'non_robust_models/cyberbullying_attacked/CW'+'/cyberbullying/'+str(i)+'.png')\n",
    "                    \n",
    "        i += 1\n",
    "        if i > 400:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "99e1fa45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ocistudent/ocistudent/envs/SMU/lib/python3.7/site-packages/PIL/Image.py:993: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n"
     ]
    }
   ],
   "source": [
    "# auto\n",
    "\n",
    "i = 0\n",
    "\n",
    "\n",
    "for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "        inputs, targets = inputs, targets\n",
    "        #print(type(targets.item()))\n",
    "\n",
    "        #adv_image = atk(inputs, targets)\n",
    "        if targets.item() == 0:\n",
    "            #Auto\n",
    "            atk = torchattacks.AutoAttack(model, eps=8/255, n_classes=2, version='standard')\n",
    "            try:\n",
    "                adv_image = atk(inputs, targets)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            torchvision.utils.save_image(adv_image, 'non_robust_models/cyberbullying_attacked/auto'+'/cyberbullying/'+str(i)+'.png')\n",
    "\n",
    "        \n",
    "        \n",
    "        if targets.item() == 1:\n",
    "            #Auto\n",
    "            atk = torchattacks.AutoAttack(model, eps=8/255, n_classes=2, version='standard')\n",
    "            try:\n",
    "                adv_image = atk(inputs, targets)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            torchvision.utils.save_image(adv_image, 'non_robust_models/cyberbullying_attacked/auto'+'/non_cyberbullying/'+str(i)+'.png')\n",
    "\n",
    "                    \n",
    "        i += 1\n",
    "        if i > 400:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c9aa7f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ocistudent/ocistudent/envs/SMU/lib/python3.7/site-packages/PIL/Image.py:993: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n"
     ]
    }
   ],
   "source": [
    "# comb\n",
    "\n",
    "i = 0\n",
    "\n",
    "\n",
    "for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "        inputs, targets = inputs, targets\n",
    "        #print(type(targets.item()))\n",
    "\n",
    "        #adv_image = atk(inputs, targets)\n",
    "        if targets.item() == 0:\n",
    "            atk1 = torchattacks.BIM(model, eps=4/255, alpha=1/255, steps=10)\n",
    "            adv_image1 = atk1(inputs, targets)\n",
    "            atk2 = torchattacks.DeepFool(model, steps=50, overshoot=0.02)\n",
    "            adv_image2 = atk2(inputs, targets)\n",
    "    \n",
    "            add = adv_image1.add(adv_image2)\n",
    "    \n",
    "            adv_image = torch.div(add, 2)\n",
    "            \n",
    "            torchvision.utils.save_image(adv_image, 'non_robust_models/cyberbullying_attacked/comb'+'/cyberbullying/'+str(i)+'.png')\n",
    "\n",
    "        \n",
    "        \n",
    "        if targets.item() == 1:\n",
    "            atk1 = torchattacks.BIM(model, eps=4/255, alpha=1/255, steps=10)\n",
    "            adv_image1 = atk1(inputs, targets)\n",
    "            atk2 = torchattacks.DeepFool(model, steps=50, overshoot=0.02)\n",
    "            adv_image2 = atk2(inputs, targets)\n",
    "    \n",
    "            add = adv_image1.add(adv_image2)\n",
    "    \n",
    "            adv_image = torch.div(add, 2)\n",
    "            \n",
    "            torchvision.utils.save_image(adv_image, 'non_robust_models/cyberbullying_attacked/comb'+'/non_cyberbullying/'+str(i)+'.png')\n",
    "\n",
    "                    \n",
    "        i += 1\n",
    "        if i > 400:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b08ef26",
   "metadata": {},
   "source": [
    "### Self-Harm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "28d60ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/ocistudent/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model import resnet\n",
    "#from model.utils import load_filtered_state_dict, SaveBestModel, AverageMeter, accuracy\n",
    "# model = resnet.ResNet(torchvision.models.resnet.Bottleneck, [3, 4, 6, 3], 2)\n",
    "# checkpoint = torch.load('nsfw-pytorch/models/nsfw/epoch_2.pkl')\n",
    "# load_filtered_state_dict(model, checkpoint, ignore_layer=[], reverse=True)\n",
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', pretrained=True)\n",
    "model.fc = nn.Linear(2048, 2, bias=True)\n",
    "checkpoint = torch.load('non_robust_models/ResNet_SH.pth')\n",
    "#model.load_state_dict(checkpoint, strict=False)\n",
    "transform_test = transforms.Compose([transforms.Resize(320),\n",
    "        transforms.RandomCrop(299), transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "good_img_transform = transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "classes = {0:'non_self_harm', 1:'self_harm'}\n",
    "\n",
    "test_path = 'self_harm/test'\n",
    "testset = torchvision.datasets.ImageFolder(test_path, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=1, shuffle=True, num_workers=2)\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "model.load_state_dict(checkpoint, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "11a16ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "\n",
    "for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "        inputs, targets = inputs, targets\n",
    "        #print(type(targets.item()))\n",
    "        if targets.item() == 0:\n",
    "             torchvision.utils.save_image(inputs, 'non_robust_models/self_harm_attacked/clean'+'/non_self_harm/'+str(i)+'.png')\n",
    "            \n",
    "        if targets.item() == 1:\n",
    "             torchvision.utils.save_image(inputs, 'non_robust_models/self_harm_attacked/clean'+'/self_harm/'+str(i)+'.png')\n",
    "         \n",
    "        i += 1\n",
    "        if i > 400:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "590a4ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "\n",
    "\n",
    "for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "        inputs, targets = inputs, targets\n",
    "        #print(type(targets.item()))\n",
    "        adv_image = atk(inputs, targets)\n",
    "        if targets.item() == 0:\n",
    "            #BIM\n",
    "            atk = torchattacks.BIM(model, eps=4/255, alpha=1/255, steps=10)\n",
    "            adv_image = atk(inputs, targets)\n",
    "            torchvision.utils.save_image(adv_image, 'non_robust_models/self_harm_attacked/BIM'+'/non_self_harm/'+str(i)+'.png')\n",
    "            #Square\n",
    "            atk = torchattacks.Square(model, norm='Linf', n_queries=500, n_restarts=1, eps=16/255, p_init=.8, seed=0, verbose=False, loss='margin', resc_schedule=True)\n",
    "            adv_image = atk(inputs, targets)\n",
    "            torchvision.utils.save_image(adv_image, 'non_robust_models/self_harm_attacked/Square'+'/non_self_harm/'+str(i)+'.png')\n",
    "            #DF\n",
    "            atk = torchattacks.DeepFool(model, steps=50, overshoot=0.02)\n",
    "            adv_image = atk(inputs, targets)\n",
    "            torchvision.utils.save_image(adv_image, 'non_robust_models/self_harm_attacked/DF'+'/non_self_harm/'+str(i)+'.png')\n",
    "            #CW\n",
    "            atk = torchattacks.CW(model, c=1e-4, kappa=0, steps=100, lr=0.01)\n",
    "            adv_image = atk(inputs, targets)\n",
    "            torchvision.utils.save_image(adv_image, 'non_robust_models/self_harm_attacked/CW'+'/non_self_harm/'+str(i)+'.png')\n",
    "            \n",
    "        if targets.item() == 1:\n",
    "            #BIM\n",
    "            atk = torchattacks.BIM(model, eps=4/255, alpha=1/255, steps=10)\n",
    "            adv_image = atk(inputs, targets)\n",
    "            torchvision.utils.save_image(adv_image, 'non_robust_models/self_harm_attacked/BIM'+'/self_harm/'+str(i)+'.png')\n",
    "            #Square\n",
    "            atk = torchattacks.Square(model, norm='Linf', n_queries=500, n_restarts=1, eps=16/255, p_init=.8, seed=0, verbose=False, loss='margin', resc_schedule=True)\n",
    "            adv_image = atk(inputs, targets)\n",
    "            torchvision.utils.save_image(adv_image, 'non_robust_models/self_harm_attacked/Square'+'/self_harm/'+str(i)+'.png')\n",
    "            #DF\n",
    "            atk = torchattacks.DeepFool(model, steps=50, overshoot=0.02)\n",
    "            adv_image = atk(inputs, targets)\n",
    "            torchvision.utils.save_image(adv_image, 'non_robust_models/self_harm_attacked/DF'+'/self_harm/'+str(i)+'.png')\n",
    "            #CW\n",
    "            atk = torchattacks.CW(model, c=1e-4, kappa=0, steps=100, lr=0.01)\n",
    "            adv_image = atk(inputs, targets)\n",
    "            torchvision.utils.save_image(adv_image, 'non_robust_models/self_harm_attacked/CW'+'/self_harm/'+str(i)+'.png')\n",
    "                    \n",
    "        i += 1\n",
    "        if i > 400:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b965a0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# auto\n",
    "\n",
    "i = 0\n",
    "\n",
    "\n",
    "for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "        inputs, targets = inputs, targets\n",
    "        #print(type(targets.item()))\n",
    "\n",
    "        #adv_image = atk(inputs, targets)\n",
    "        if targets.item() == 0:\n",
    "            #Auto\n",
    "            atk = torchattacks.AutoAttack(model, eps=8/255, n_classes=2, version='standard')\n",
    "            try:\n",
    "                adv_image = atk(inputs, targets)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            torchvision.utils.save_image(adv_image, 'non_robust_models/self_harm_attacked/auto'+'/non_self_harm/'+str(i)+'.png')\n",
    "\n",
    "        \n",
    "        \n",
    "        if targets.item() == 1:\n",
    "            #Auto\n",
    "            atk = torchattacks.AutoAttack(model, eps=8/255, n_classes=2, version='standard')\n",
    "            try:\n",
    "                adv_image = atk(inputs, targets)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            torchvision.utils.save_image(adv_image, 'non_robust_models/self_harm_attacked/auto'+'/self_harm/'+str(i)+'.png')\n",
    "\n",
    "                    \n",
    "        i += 1\n",
    "        if i > 400:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5d1d2fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comb\n",
    "\n",
    "i = 0\n",
    "\n",
    "\n",
    "for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "        inputs, targets = inputs, targets\n",
    "        #print(type(targets.item()))\n",
    "\n",
    "        #adv_image = atk(inputs, targets)\n",
    "        if targets.item() == 0:\n",
    "            atk1 = torchattacks.BIM(model, eps=4/255, alpha=1/255, steps=10)\n",
    "            adv_image1 = atk1(inputs, targets)\n",
    "            atk2 = torchattacks.DeepFool(model, steps=50, overshoot=0.02)\n",
    "            adv_image2 = atk2(inputs, targets)\n",
    "    \n",
    "            add = adv_image1.add(adv_image2)\n",
    "    \n",
    "            adv_image = torch.div(add, 2)\n",
    "            \n",
    "            torchvision.utils.save_image(adv_image, 'non_robust_models/self_harm_attacked/comb'+'/non_self_harm/'+str(i)+'.png')\n",
    "\n",
    "        \n",
    "        \n",
    "        if targets.item() == 1:\n",
    "            atk1 = torchattacks.BIM(model, eps=4/255, alpha=1/255, steps=10)\n",
    "            adv_image1 = atk1(inputs, targets)\n",
    "            atk2 = torchattacks.DeepFool(model, steps=50, overshoot=0.02)\n",
    "            adv_image2 = atk2(inputs, targets)\n",
    "    \n",
    "            add = adv_image1.add(adv_image2)\n",
    "    \n",
    "            adv_image = torch.div(add, 2)\n",
    "            \n",
    "            torchvision.utils.save_image(adv_image, 'non_robust_models/self_harm_attacked/comb'+'/self_harm/'+str(i)+'.png')\n",
    "\n",
    "                    \n",
    "        i += 1\n",
    "        if i > 400:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eabb570",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7672882e",
   "metadata": {},
   "source": [
    "## Robust Model Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c0a3cb",
   "metadata": {},
   "source": [
    "### NSFW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3e0219e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/ocistudent/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['fc.weight', 'fc.bias'], unexpected_keys=['fc.fc.weight', 'fc.fc.bias'])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', pretrained=True)\n",
    "model.fc = nn.Linear(2048, 2, bias=True)\n",
    "checkpoint = torch.load('robust_models/robust_NSFW.pth')\n",
    "#model.load_state_dict(checkpoint, strict=False)\n",
    "transform_test = transforms.Compose([transforms.Resize(320),\n",
    "        transforms.RandomCrop(299), transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "good_img_transform = transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "classes = {0:'neutral', 1:'porn'}\n",
    "\n",
    "test_path = 'nsfw/test'\n",
    "testset = torchvision.datasets.ImageFolder(test_path, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=1, shuffle=True, num_workers=2)\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "model.load_state_dict(checkpoint, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7254d36b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from modules import *\n",
    "model = resnet50()\n",
    "checkpoint = torch.load('robust_models/robust_NSFW.pth')\n",
    "#model.load_state_dict(checkpoint, strict=False)\n",
    "transform_test = transforms.Compose([transforms.Resize(320),\n",
    "        transforms.RandomCrop(299), transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "transform_test = transforms.Compose([transforms.Resize(320),\n",
    "        transforms.RandomCrop(299), transforms.ToTensor()])\n",
    "\n",
    "good_img_transform = transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "classes = {0:'neutral', 1:'porn'}\n",
    "\n",
    "test_path = 'nsfw/test'\n",
    "testset = torchvision.datasets.ImageFolder(test_path, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=1, shuffle=True, num_workers=2)\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "model.load_state_dict(checkpoint, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "79645096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of robust model on unattacked data: 0.9006622516556292\n"
     ]
    }
   ],
   "source": [
    "total_samples = 0\n",
    "correct = 0\n",
    "for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "    inputs, targets = inputs, targets\n",
    "    \n",
    "    label = targets\n",
    "        \n",
    "    outputs = model(inputs.to(device))\n",
    "    _, predicted = outputs.max(1)\n",
    "    \n",
    "    \n",
    "    pred_val = predicted.cpu()\n",
    "    \n",
    "    total_samples += 1\n",
    "    if pred_val == label:\n",
    "        correct += 1\n",
    "    \n",
    "    if total_samples > 150:\n",
    "        break\n",
    "    \n",
    "print('Accuracy of robust model on unattacked data: '+str(correct / total_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9f9bedf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of robust model on BIM: 0.9403973509933775\n"
     ]
    }
   ],
   "source": [
    "test_path = 'non_robust_models/NSFW_attacked/Square'\n",
    "\n",
    "transform_test = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "testset = torchvision.datasets.ImageFolder(test_path, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=1, shuffle=True, num_workers=2)\n",
    "\n",
    "total_samples = 0\n",
    "correct = 0\n",
    "for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "    inputs, targets = inputs, targets\n",
    "    \n",
    "    label = targets\n",
    "        \n",
    "    outputs = model(inputs.to(device))\n",
    "    _, predicted = outputs.max(1)\n",
    "    \n",
    "    \n",
    "    pred_val = predicted.cpu()\n",
    "    \n",
    "    total_samples += 1\n",
    "    if pred_val == label:\n",
    "        correct += 1\n",
    "    \n",
    "    if total_samples > 150:\n",
    "        break\n",
    "    \n",
    "print('Accuracy of robust model on BIM: '+str(correct / total_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c988956",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8ad74f86",
   "metadata": {},
   "source": [
    "### Cyberbullying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "7d3302c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320 320\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AvgPool2d(kernel_size=7, stride=7, padding=0)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (fc_angles): Linear(in_features=2048, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model import resnet\n",
    "from model.utils import load_filtered_state_dict, SaveBestModel, AverageMeter, accuracy\n",
    "model = resnet.ResNet(torchvision.models.resnet.Bottleneck, [3, 4, 6, 3], 2)\n",
    "checkpoint = torch.load('robust_models/robust_CB.pth')\n",
    "load_filtered_state_dict(model, checkpoint, ignore_layer=[], reverse=True)\n",
    "transform_test = transforms.Compose([transforms.Resize(320),\n",
    "        transforms.RandomCrop(299), transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "good_img_transform = transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "classes = {0:'non_self_harm', 1:'self_harm'}\n",
    "\n",
    "test_path = 'cyberbullying/test'\n",
    "testset = torchvision.datasets.ImageFolder(test_path, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=1, shuffle=True, num_workers=2)\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "#model.load_state_dict(checkpoint, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "ac69f6bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ocistudent/ocistudent/envs/SMU/lib/python3.7/site-packages/PIL/Image.py:993: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of robust model on unattacked data: 0.8874172185430463\n"
     ]
    }
   ],
   "source": [
    "total_samples = 0\n",
    "correct = 0\n",
    "for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "    inputs, targets = inputs, targets\n",
    "    #print(targets)\n",
    "    label = targets\n",
    "        \n",
    "    outputs = model(inputs.to(device))\n",
    "    _, predicted = outputs.max(1)\n",
    "    \n",
    "    \n",
    "    pred_val = predicted.cpu()\n",
    "    \n",
    "    total_samples += 1\n",
    "    if pred_val == label:\n",
    "        correct += 1\n",
    "    \n",
    "    if total_samples > 150:\n",
    "        break\n",
    "    \n",
    "print('Accuracy of robust model on unattacked data: '+str(correct / total_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5d5a4bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of robust model on BIM: 0.9205298013245033\n"
     ]
    }
   ],
   "source": [
    "test_path = 'non_robust_models/self_harm_attacked/BIM'\n",
    "\n",
    "transform_test = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "testset = torchvision.datasets.ImageFolder(test_path, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=1, shuffle=True, num_workers=2)\n",
    "\n",
    "total_samples = 0\n",
    "correct = 0\n",
    "for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "    inputs, targets = inputs, targets\n",
    "    \n",
    "    label = targets\n",
    "        \n",
    "    outputs = model(inputs.to(device))\n",
    "    _, predicted = outputs.max(1)\n",
    "    \n",
    "    \n",
    "    pred_val = predicted.cpu()\n",
    "    \n",
    "    total_samples += 1\n",
    "    if pred_val == label:\n",
    "        correct += 1\n",
    "    \n",
    "    if total_samples > 150:\n",
    "        break\n",
    "    \n",
    "print('Accuracy of robust model on BIM: '+str(correct / total_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d382dcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f028dbd9",
   "metadata": {},
   "source": [
    "### Self-Harm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "0764a98e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320 320\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AvgPool2d(kernel_size=7, stride=7, padding=0)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (fc_angles): Linear(in_features=2048, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model import resnet\n",
    "from model.utils import load_filtered_state_dict, SaveBestModel, AverageMeter, accuracy\n",
    "model = resnet.ResNet(torchvision.models.resnet.Bottleneck, [3, 4, 6, 3], 2)\n",
    "checkpoint = torch.load('robust_models/robust_SH.pth')\n",
    "load_filtered_state_dict(model, checkpoint, ignore_layer=[], reverse=True)\n",
    "transform_test = transforms.Compose([transforms.Resize(320),\n",
    "        transforms.RandomCrop(299), transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "good_img_transform = transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "classes = {0:'non_self_harm', 1:'self_harm'}\n",
    "\n",
    "test_path = 'self_harm/test'\n",
    "testset = torchvision.datasets.ImageFolder(test_path, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=1, shuffle=True, num_workers=2)\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "#model.load_state_dict(checkpoint, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ff809024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of robust model on unattacked data: 0.9139072847682119\n"
     ]
    }
   ],
   "source": [
    "total_samples = 0\n",
    "correct = 0\n",
    "for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "    inputs, targets = inputs, targets\n",
    "    #print(targets)\n",
    "    label = targets\n",
    "        \n",
    "    outputs = model(inputs.to(device))\n",
    "    _, predicted = outputs.max(1)\n",
    "    \n",
    "    \n",
    "    pred_val = predicted.cpu()\n",
    "    \n",
    "    total_samples += 1\n",
    "    if pred_val == label:\n",
    "        correct += 1\n",
    "    \n",
    "    if total_samples > 150:\n",
    "        break\n",
    "    \n",
    "print('Accuracy of robust model on unattacked data: '+str(correct / total_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b996c720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of robust model on BIM: 0.9205298013245033\n"
     ]
    }
   ],
   "source": [
    "test_path = 'non_robust_models/self_harm_attacked/BIM'\n",
    "\n",
    "transform_test = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "testset = torchvision.datasets.ImageFolder(test_path, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=1, shuffle=True, num_workers=2)\n",
    "\n",
    "total_samples = 0\n",
    "correct = 0\n",
    "for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "    inputs, targets = inputs, targets\n",
    "    \n",
    "    label = targets\n",
    "        \n",
    "    outputs = model(inputs.to(device))\n",
    "    _, predicted = outputs.max(1)\n",
    "    \n",
    "    \n",
    "    pred_val = predicted.cpu()\n",
    "    \n",
    "    total_samples += 1\n",
    "    if pred_val == label:\n",
    "        correct += 1\n",
    "    \n",
    "    if total_samples > 150:\n",
    "        break\n",
    "    \n",
    "print('Accuracy of robust model on BIM: '+str(correct / total_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92645e0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b2929ae",
   "metadata": {},
   "source": [
    "## Evaluation of Robust Models with Purified Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b7c18b",
   "metadata": {},
   "source": [
    "### NSFW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0c966137",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from modules import *\n",
    "model = resnet50()\n",
    "checkpoint = torch.load('robust_models/robust_NSFW.pth')\n",
    "#model.load_state_dict(checkpoint, strict=False)\n",
    "transform_test = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "good_img_transform = transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "classes = {0:'neutral', 1:'porn'}\n",
    "\n",
    "test_path = 'nsfw/test'\n",
    "testset = torchvision.datasets.ImageFolder(test_path, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=1, shuffle=True, num_workers=2)\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "model.load_state_dict(checkpoint, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bb6f073e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of robust model on Purified clean: 0.8741721854304636\n"
     ]
    }
   ],
   "source": [
    "test_path = 'purified_images/NSFW_purified/clean'\n",
    "testset = torchvision.datasets.ImageFolder(test_path, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=1, shuffle=True, num_workers=2)\n",
    "\n",
    "total_samples = 0\n",
    "correct = 0\n",
    "for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "    inputs, targets = inputs, targets\n",
    "    \n",
    "    label = targets\n",
    "        \n",
    "    outputs = model(inputs.to(device))\n",
    "    _, predicted = outputs.max(1)\n",
    "    \n",
    "    \n",
    "    pred_val = predicted.cpu()\n",
    "    \n",
    "    total_samples += 1\n",
    "    if pred_val == label:\n",
    "        correct += 1\n",
    "    \n",
    "    if total_samples > 150:\n",
    "        break\n",
    "    \n",
    "print('Accuracy of robust model on Purified clean: '+str(correct / total_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a41e0a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of robust model on Purified BIM: 0.8874172185430463\n"
     ]
    }
   ],
   "source": [
    "test_path = 'purified_images/NSFW_purified/BIM'\n",
    "testset = torchvision.datasets.ImageFolder(test_path, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=1, shuffle=True, num_workers=2)\n",
    "\n",
    "total_samples = 0\n",
    "correct = 0\n",
    "for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "    inputs, targets = inputs, targets\n",
    "    \n",
    "    label = targets\n",
    "        \n",
    "    outputs = model(inputs.to(device))\n",
    "    _, predicted = outputs.max(1)\n",
    "    \n",
    "    \n",
    "    pred_val = predicted.cpu()\n",
    "    \n",
    "    total_samples += 1\n",
    "    if pred_val == label:\n",
    "        correct += 1\n",
    "    \n",
    "    if total_samples > 150:\n",
    "        break\n",
    "    \n",
    "print('Accuracy of robust model on Purified BIM: '+str(correct / total_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e6cbaf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of robust model on Purified Square: 0.8807947019867549\n"
     ]
    }
   ],
   "source": [
    "test_path = 'purified_images/NSFW_purified/Square'\n",
    "testset = torchvision.datasets.ImageFolder(test_path, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=1, shuffle=True, num_workers=2)\n",
    "\n",
    "total_samples = 0\n",
    "correct = 0\n",
    "for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "    inputs, targets = inputs, targets\n",
    "    \n",
    "    label = targets\n",
    "        \n",
    "    outputs = model(inputs.to(device))\n",
    "    _, predicted = outputs.max(1)\n",
    "    \n",
    "    \n",
    "    pred_val = predicted.cpu()\n",
    "    \n",
    "    total_samples += 1\n",
    "    if pred_val == label:\n",
    "        correct += 1\n",
    "    \n",
    "    if total_samples > 150:\n",
    "        break\n",
    "    \n",
    "print('Accuracy of robust model on Purified Square: '+str(correct / total_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8248c39c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of robust model on Purified CW: 0.7417218543046358\n"
     ]
    }
   ],
   "source": [
    "test_path = 'purified_images/NSFW_purified/CW'\n",
    "testset = torchvision.datasets.ImageFolder(test_path, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=1, shuffle=True, num_workers=2)\n",
    "\n",
    "total_samples = 0\n",
    "correct = 0\n",
    "for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "    inputs, targets = inputs, targets\n",
    "    \n",
    "    label = targets\n",
    "        \n",
    "    outputs = model(inputs.to(device))\n",
    "    _, predicted = outputs.max(1)\n",
    "    \n",
    "    \n",
    "    pred_val = predicted.cpu()\n",
    "    \n",
    "    total_samples += 1\n",
    "    if pred_val == label:\n",
    "        correct += 1\n",
    "    \n",
    "    if total_samples > 150:\n",
    "        break\n",
    "    \n",
    "print('Accuracy of robust model on Purified CW: '+str(correct / total_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3edca30f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of robust model on Purified DF: 0.8807947019867549\n"
     ]
    }
   ],
   "source": [
    "test_path = 'purified_images/NSFW_purified/DF'\n",
    "testset = torchvision.datasets.ImageFolder(test_path, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=1, shuffle=True, num_workers=2)\n",
    "\n",
    "total_samples = 0\n",
    "correct = 0\n",
    "for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "    inputs, targets = inputs, targets\n",
    "    \n",
    "    label = targets\n",
    "        \n",
    "    outputs = model(inputs.to(device))\n",
    "    _, predicted = outputs.max(1)\n",
    "    \n",
    "    \n",
    "    pred_val = predicted.cpu()\n",
    "    \n",
    "    total_samples += 1\n",
    "    if pred_val == label:\n",
    "        correct += 1\n",
    "    \n",
    "    if total_samples > 150:\n",
    "        break\n",
    "    \n",
    "print('Accuracy of robust model on Purified DF: '+str(correct / total_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d0e57768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of robust model on Purified auto: 0.8410596026490066\n"
     ]
    }
   ],
   "source": [
    "test_path = 'purified_images/NSFW_purified/auto'\n",
    "testset = torchvision.datasets.ImageFolder(test_path, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=1, shuffle=True, num_workers=2)\n",
    "\n",
    "total_samples = 0\n",
    "correct = 0\n",
    "for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "    inputs, targets = inputs, targets\n",
    "    \n",
    "    label = targets\n",
    "        \n",
    "    outputs = model(inputs.to(device))\n",
    "    _, predicted = outputs.max(1)\n",
    "    \n",
    "    \n",
    "    pred_val = predicted.cpu()\n",
    "    \n",
    "    total_samples += 1\n",
    "    if pred_val == label:\n",
    "        correct += 1\n",
    "    \n",
    "    if total_samples > 150:\n",
    "        break\n",
    "    \n",
    "print('Accuracy of robust model on Purified auto: '+str(correct / total_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1b79328c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of robust model on Purified comb: 0.8940397350993378\n"
     ]
    }
   ],
   "source": [
    "test_path = 'purified_images/NSFW_purified/comb'\n",
    "testset = torchvision.datasets.ImageFolder(test_path, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=1, shuffle=True, num_workers=2)\n",
    "\n",
    "total_samples = 0\n",
    "correct = 0\n",
    "for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "    inputs, targets = inputs, targets\n",
    "    \n",
    "    label = targets\n",
    "        \n",
    "    outputs = model(inputs.to(device))\n",
    "    _, predicted = outputs.max(1)\n",
    "    \n",
    "    \n",
    "    pred_val = predicted.cpu()\n",
    "    \n",
    "    total_samples += 1\n",
    "    if pred_val == label:\n",
    "        correct += 1\n",
    "    \n",
    "    if total_samples > 150:\n",
    "        break\n",
    "    \n",
    "print('Accuracy of robust model on Purified comb: '+str(correct / total_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85285f1",
   "metadata": {},
   "source": [
    "### Cyberbullying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "48b2293a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320 320\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AvgPool2d(kernel_size=7, stride=7, padding=0)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (fc_angles): Linear(in_features=2048, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model import resnet\n",
    "from model.utils import load_filtered_state_dict, SaveBestModel, AverageMeter, accuracy\n",
    "model = resnet.ResNet(torchvision.models.resnet.Bottleneck, [3, 4, 6, 3], 2)\n",
    "checkpoint = torch.load('robust_models/robust_CB.pth')\n",
    "load_filtered_state_dict(model, checkpoint, ignore_layer=[], reverse=True)\n",
    "transform_test = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "good_img_transform = transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "classes = {0:'non_self_harm', 1:'self_harm'}\n",
    "\n",
    "test_path = 'cyberbullying/test'\n",
    "testset = torchvision.datasets.ImageFolder(test_path, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=1, shuffle=True, num_workers=2)\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "#model.load_state_dict(checkpoint, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18d91516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of robust model on Purified clean: 0.9602649006622517\n"
     ]
    }
   ],
   "source": [
    "test_path = 'purified_images/cyberbullying_purified/clean'\n",
    "testset = torchvision.datasets.ImageFolder(test_path, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=1, shuffle=True, num_workers=2)\n",
    "\n",
    "total_samples = 0\n",
    "correct = 0\n",
    "for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "    inputs, targets = inputs, targets\n",
    "    \n",
    "    label = targets\n",
    "        \n",
    "    outputs = model(inputs.to(device))\n",
    "    _, predicted = outputs.max(1)\n",
    "    \n",
    "    \n",
    "    pred_val = predicted.cpu()\n",
    "    \n",
    "    total_samples += 1\n",
    "    if pred_val == label:\n",
    "        correct += 1\n",
    "    \n",
    "    if total_samples > 150:\n",
    "        break\n",
    "    \n",
    "print('Accuracy of robust model on Purified clean: '+str(correct / total_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f94375a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of robust model on Purified BIM: 0.9470198675496688\n"
     ]
    }
   ],
   "source": [
    "test_path = 'purified_images/cyberbullying_purified/BIM'\n",
    "testset = torchvision.datasets.ImageFolder(test_path, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=1, shuffle=True, num_workers=2)\n",
    "\n",
    "total_samples = 0\n",
    "correct = 0\n",
    "for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "    inputs, targets = inputs, targets\n",
    "    \n",
    "    label = targets\n",
    "        \n",
    "    outputs = model(inputs.to(device))\n",
    "    _, predicted = outputs.max(1)\n",
    "    \n",
    "    \n",
    "    pred_val = predicted.cpu()\n",
    "    \n",
    "    total_samples += 1\n",
    "    if pred_val == label:\n",
    "        correct += 1\n",
    "    \n",
    "    if total_samples > 150:\n",
    "        break\n",
    "    \n",
    "print('Accuracy of robust model on Purified BIM: '+str(correct / total_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ef8e1d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of robust model on Purified Square: 0.9536423841059603\n"
     ]
    }
   ],
   "source": [
    "test_path = 'purified_images/cyberbullying_purified/Square'\n",
    "testset = torchvision.datasets.ImageFolder(test_path, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=1, shuffle=True, num_workers=2)\n",
    "\n",
    "total_samples = 0\n",
    "correct = 0\n",
    "for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "    inputs, targets = inputs, targets\n",
    "    \n",
    "    label = targets\n",
    "        \n",
    "    outputs = model(inputs.to(device))\n",
    "    _, predicted = outputs.max(1)\n",
    "    \n",
    "    \n",
    "    pred_val = predicted.cpu()\n",
    "    \n",
    "    total_samples += 1\n",
    "    if pred_val == label:\n",
    "        correct += 1\n",
    "    \n",
    "    if total_samples > 150:\n",
    "        break\n",
    "    \n",
    "print('Accuracy of robust model on Purified Square: '+str(correct / total_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31970731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of robust model on Purified CW: 0.8344370860927153\n"
     ]
    }
   ],
   "source": [
    "test_path = 'purified_images/cyberbullying_purified/CW'\n",
    "testset = torchvision.datasets.ImageFolder(test_path, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=1, shuffle=True, num_workers=2)\n",
    "\n",
    "total_samples = 0\n",
    "correct = 0\n",
    "for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "    inputs, targets = inputs, targets\n",
    "    \n",
    "    label = targets\n",
    "        \n",
    "    outputs = model(inputs.to(device))\n",
    "    _, predicted = outputs.max(1)\n",
    "    \n",
    "    \n",
    "    pred_val = predicted.cpu()\n",
    "    \n",
    "    total_samples += 1\n",
    "    if pred_val == label:\n",
    "        correct += 1\n",
    "    \n",
    "    if total_samples > 150:\n",
    "        break\n",
    "    \n",
    "print('Accuracy of robust model on Purified CW: '+str(correct / total_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "242f6a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of robust model on Purified DF: 0.9271523178807947\n"
     ]
    }
   ],
   "source": [
    "test_path = 'purified_images/cyberbullying_purified/DF'\n",
    "testset = torchvision.datasets.ImageFolder(test_path, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=1, shuffle=True, num_workers=2)\n",
    "\n",
    "total_samples = 0\n",
    "correct = 0\n",
    "for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "    inputs, targets = inputs, targets\n",
    "    \n",
    "    label = targets\n",
    "        \n",
    "    outputs = model(inputs.to(device))\n",
    "    _, predicted = outputs.max(1)\n",
    "    \n",
    "    \n",
    "    pred_val = predicted.cpu()\n",
    "    \n",
    "    total_samples += 1\n",
    "    if pred_val == label:\n",
    "        correct += 1\n",
    "    \n",
    "    if total_samples > 150:\n",
    "        break\n",
    "    \n",
    "print('Accuracy of robust model on Purified DF: '+str(correct / total_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7b5c184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of robust model on Purified auto: 0.9072847682119205\n"
     ]
    }
   ],
   "source": [
    "test_path = 'purified_images/cyberbullying_purified/auto'\n",
    "testset = torchvision.datasets.ImageFolder(test_path, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=1, shuffle=True, num_workers=2)\n",
    "\n",
    "total_samples = 0\n",
    "correct = 0\n",
    "for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "    inputs, targets = inputs, targets\n",
    "    \n",
    "    label = targets\n",
    "        \n",
    "    outputs = model(inputs.to(device))\n",
    "    _, predicted = outputs.max(1)\n",
    "    \n",
    "    \n",
    "    pred_val = predicted.cpu()\n",
    "    \n",
    "    total_samples += 1\n",
    "    if pred_val == label:\n",
    "        correct += 1\n",
    "    \n",
    "    if total_samples > 150:\n",
    "        break\n",
    "    \n",
    "print('Accuracy of robust model on Purified auto: '+str(correct / total_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b08b9197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of robust model on Purified comb: 0.9337748344370861\n"
     ]
    }
   ],
   "source": [
    "test_path = 'purified_images/cyberbullying_purified/comb'\n",
    "testset = torchvision.datasets.ImageFolder(test_path, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=1, shuffle=True, num_workers=2)\n",
    "\n",
    "total_samples = 0\n",
    "correct = 0\n",
    "for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "    inputs, targets = inputs, targets\n",
    "    \n",
    "    label = targets\n",
    "        \n",
    "    outputs = model(inputs.to(device))\n",
    "    _, predicted = outputs.max(1)\n",
    "    \n",
    "    \n",
    "    pred_val = predicted.cpu()\n",
    "    \n",
    "    total_samples += 1\n",
    "    if pred_val == label:\n",
    "        correct += 1\n",
    "    \n",
    "    if total_samples > 150:\n",
    "        break\n",
    "    \n",
    "print('Accuracy of robust model on Purified comb: '+str(correct / total_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860d9066",
   "metadata": {},
   "source": [
    "### Self-harm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9479c606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320 320\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AvgPool2d(kernel_size=7, stride=7, padding=0)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (fc_angles): Linear(in_features=2048, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model import resnet\n",
    "from model.utils import load_filtered_state_dict, SaveBestModel, AverageMeter, accuracy\n",
    "model = resnet.ResNet(torchvision.models.resnet.Bottleneck, [3, 4, 6, 3], 2)\n",
    "checkpoint = torch.load('robust_models/robust_SH.pth')\n",
    "load_filtered_state_dict(model, checkpoint, ignore_layer=[], reverse=True)\n",
    "transform_test = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "classes = {0:'non_self_harm', 1:'self_harm'}\n",
    "\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "#model.load_state_dict(checkpoint, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3bf5823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of robust model on Purified clean: 0.9072847682119205\n"
     ]
    }
   ],
   "source": [
    "test_path = 'purified_images/self_harm_purified/clean'\n",
    "testset = torchvision.datasets.ImageFolder(test_path, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=1, shuffle=True, num_workers=2)\n",
    "\n",
    "total_samples = 0\n",
    "correct = 0\n",
    "for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "    inputs, targets = inputs, targets\n",
    "    \n",
    "    label = targets\n",
    "        \n",
    "    outputs = model(inputs.to(device))\n",
    "    _, predicted = outputs.max(1)\n",
    "    \n",
    "    \n",
    "    pred_val = predicted.cpu()\n",
    "    \n",
    "    total_samples += 1\n",
    "    if pred_val == label:\n",
    "        correct += 1\n",
    "    \n",
    "    if total_samples > 150:\n",
    "        break\n",
    "    \n",
    "print('Accuracy of robust model on Purified clean: '+str(correct / total_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "858fcde7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of robust model on Purified BIM: 0.8874172185430463\n"
     ]
    }
   ],
   "source": [
    "test_path = 'purified_images/self_harm_purified/BIM'\n",
    "testset = torchvision.datasets.ImageFolder(test_path, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=1, shuffle=True, num_workers=2)\n",
    "\n",
    "total_samples = 0\n",
    "correct = 0\n",
    "for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "    inputs, targets = inputs, targets\n",
    "    \n",
    "    label = targets\n",
    "        \n",
    "    outputs = model(inputs.to(device))\n",
    "    _, predicted = outputs.max(1)\n",
    "    \n",
    "    \n",
    "    pred_val = predicted.cpu()\n",
    "    \n",
    "    total_samples += 1\n",
    "    if pred_val == label:\n",
    "        correct += 1\n",
    "    \n",
    "    if total_samples > 150:\n",
    "        break\n",
    "    \n",
    "print('Accuracy of robust model on Purified BIM: '+str(correct / total_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5bcc15d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of robust model on Purified Square: 0.9006622516556292\n"
     ]
    }
   ],
   "source": [
    "test_path = 'purified_images/self_harm_purified/Square'\n",
    "testset = torchvision.datasets.ImageFolder(test_path, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=1, shuffle=True, num_workers=2)\n",
    "\n",
    "total_samples = 0\n",
    "correct = 0\n",
    "for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "    inputs, targets = inputs, targets\n",
    "    \n",
    "    label = targets\n",
    "        \n",
    "    outputs = model(inputs.to(device))\n",
    "    _, predicted = outputs.max(1)\n",
    "    \n",
    "    \n",
    "    pred_val = predicted.cpu()\n",
    "    \n",
    "    total_samples += 1\n",
    "    if pred_val == label:\n",
    "        correct += 1\n",
    "    \n",
    "    if total_samples > 150:\n",
    "        break\n",
    "    \n",
    "print('Accuracy of robust model on Purified Square: '+str(correct / total_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee3287f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of robust model on Purified CW: 0.7019867549668874\n"
     ]
    }
   ],
   "source": [
    "test_path = 'purified_images/self_harm_purified/CW'\n",
    "testset = torchvision.datasets.ImageFolder(test_path, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=1, shuffle=True, num_workers=2)\n",
    "\n",
    "total_samples = 0\n",
    "correct = 0\n",
    "for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "    inputs, targets = inputs, targets\n",
    "    \n",
    "    label = targets\n",
    "        \n",
    "    outputs = model(inputs.to(device))\n",
    "    _, predicted = outputs.max(1)\n",
    "    \n",
    "    \n",
    "    pred_val = predicted.cpu()\n",
    "    \n",
    "    total_samples += 1\n",
    "    if pred_val == label:\n",
    "        correct += 1\n",
    "    \n",
    "    if total_samples > 150:\n",
    "        break\n",
    "    \n",
    "print('Accuracy of robust model on Purified CW: '+str(correct / total_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "afbed427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of robust model on Purified DF: 0.9072847682119205\n"
     ]
    }
   ],
   "source": [
    "test_path = 'purified_images/self_harm_purified/DF'\n",
    "testset = torchvision.datasets.ImageFolder(test_path, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=1, shuffle=True, num_workers=2)\n",
    "\n",
    "total_samples = 0\n",
    "correct = 0\n",
    "for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "    inputs, targets = inputs, targets\n",
    "    \n",
    "    label = targets\n",
    "        \n",
    "    outputs = model(inputs.to(device))\n",
    "    _, predicted = outputs.max(1)\n",
    "    \n",
    "    \n",
    "    pred_val = predicted.cpu()\n",
    "    \n",
    "    total_samples += 1\n",
    "    if pred_val == label:\n",
    "        correct += 1\n",
    "    \n",
    "    if total_samples > 150:\n",
    "        break\n",
    "    \n",
    "print('Accuracy of robust model on Purified DF: '+str(correct / total_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "944d2baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of robust model on Purified auto: 0.8675496688741722\n"
     ]
    }
   ],
   "source": [
    "test_path = 'purified_images/self_harm_purified/auto'\n",
    "testset = torchvision.datasets.ImageFolder(test_path, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=1, shuffle=True, num_workers=2)\n",
    "\n",
    "total_samples = 0\n",
    "correct = 0\n",
    "for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "    inputs, targets = inputs, targets\n",
    "    \n",
    "    label = targets\n",
    "        \n",
    "    outputs = model(inputs.to(device))\n",
    "    _, predicted = outputs.max(1)\n",
    "    \n",
    "    \n",
    "    pred_val = predicted.cpu()\n",
    "    \n",
    "    total_samples += 1\n",
    "    if pred_val == label:\n",
    "        correct += 1\n",
    "    \n",
    "    if total_samples > 150:\n",
    "        break\n",
    "    \n",
    "print('Accuracy of robust model on Purified auto: '+str(correct / total_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3a21d70c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of robust model on Purified comb: 0.9205298013245033\n"
     ]
    }
   ],
   "source": [
    "test_path = 'purified_images/self_harm_purified/comb'\n",
    "testset = torchvision.datasets.ImageFolder(test_path, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=1, shuffle=True, num_workers=2)\n",
    "\n",
    "total_samples = 0\n",
    "correct = 0\n",
    "for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "    inputs, targets = inputs, targets\n",
    "    \n",
    "    label = targets\n",
    "        \n",
    "    outputs = model(inputs.to(device))\n",
    "    _, predicted = outputs.max(1)\n",
    "    \n",
    "    \n",
    "    pred_val = predicted.cpu()\n",
    "    \n",
    "    total_samples += 1\n",
    "    if pred_val == label:\n",
    "        correct += 1\n",
    "    \n",
    "    if total_samples > 150:\n",
    "        break\n",
    "    \n",
    "print('Accuracy of robust model on Purified comb: '+str(correct / total_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196e179a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
